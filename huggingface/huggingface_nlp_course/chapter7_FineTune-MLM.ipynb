{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = r\"D:\\huggingface\\distilbert\\distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT number of parameters:67M\n",
      "BERT number of parameters:110M\n"
     ]
    }
   ],
   "source": [
    "distilbert_num_parameters = model.num_parameters() / 1000000\n",
    "print(f\"DistilBERT number of parameters:{round(distilbert_num_parameters)}M\")\n",
    "print(f\"BERT number of parameters:110M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "torch.Size([1, 8, 30522])\n",
      "torch.Size([1, 30522])\n",
      "This is a great deal.\n",
      "This is a great success.\n",
      "This is a great adventure.\n",
      "This is a great idea.\n",
      "This is a great feat.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a great [MASK].\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**inputs).logits\n",
    "print(inputs['input_ids'].shape)\n",
    "print(token_logits.shape)\n",
    "\n",
    "\n",
    "mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "print(mask_token_logits.shape)\n",
    "\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "for token in top_5_tokens:\n",
    "    print(f\"{text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_path = r'D:\\huggingface\\datasets\\imdb'\n",
    "# imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset = load_from_disk(dataset_path)\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = imdb_dataset['train'].shuffle(seed=42).select(range(3))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...\n",
      "Label:1\n",
      "Review:This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.\n",
      "Label:1\n",
      "Review:George P. Cosmatos' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn't win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.\n",
      "Label:0\n"
     ]
    }
   ],
   "source": [
    "for row in sample:\n",
    "    print(f\"Review:{row['text']}\")\n",
    "    print(f\"Label:{row['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'This is just a precious little diamond. The play, the script are excellent. I cant compare this movie with anything else, maybe except the movie \"Leon\" wonderfully played by Jean Reno and Natalie Portman. But... What can I say about this one? This is the best movie Anne Parillaud has ever played in (See please \"Frankie Starlight\", she\\'s speaking English there) to see what I mean. The story of young punk girl Nikita, taken into the depraved world of the secret government forces has been exceptionally over used by Americans. Never mind the \"Point of no return\" and especially the \"La femme Nikita\" TV series. They cannot compare the original believe me! Trash these videos. Buy this one, do not rent it, BUY it. BTW beware of the subtitles of the LA company which \"translate\" the US release. What a disgrace! If you cant understand French, get a dubbed version. But you\\'ll regret later :)',\n",
       " 'label': -1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['unsupervised'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba336286a71c4793a43c8af849f73c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e273395639645179eb80cf97256e7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37f295989b24865ae5358a63829a48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples['text']) # without trunction=True\n",
    "    if tokenizer.is_fast:\n",
    "        result['word_ids'] = [result.word_ids(i) for i in range(len(result['input_ids']))]\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=['text', 'label']\n",
    ")\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 length:107\n",
      "Review 1 length:150\n",
      "Review 2 length:112\n",
      "Review 3 length:640\n",
      "Review 4 length:235\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 128\n",
    "\n",
    "tokenized_samples = tokenized_datasets['train'][210:215]\n",
    "for idx, sample in enumerate(tokenized_samples['input_ids']):\n",
    "    print(f\"Review {idx} length:{len(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated reviews length:1244\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k:sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "\n",
    "total_length = len(concatenated_examples['input_ids'])\n",
    "print(f\"Concatenated reviews length:{total_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  1037,\n",
       "  11519,\n",
       "  8297,\n",
       "  1010,\n",
       "  2021,\n",
       "  2515,\n",
       "  2025,\n",
       "  5308,\n",
       "  1996,\n",
       "  8595,\n",
       "  1997,\n",
       "  1996,\n",
       "  2434,\n",
       "  1012,\n",
       "  1037,\n",
       "  25303,\n",
       "  11167,\n",
       "  1006,\n",
       "  20128,\n",
       "  5912,\n",
       "  1007,\n",
       "  15980,\n",
       "  2047,\n",
       "  15702,\n",
       "  1999,\n",
       "  2344,\n",
       "  2000,\n",
       "  3622,\n",
       "  2010,\n",
       "  2219,\n",
       "  3117,\n",
       "  6644,\n",
       "  2011,\n",
       "  1996,\n",
       "  2697,\n",
       "  1012,\n",
       "  2145,\n",
       "  18101,\n",
       "  4288,\n",
       "  1010,\n",
       "  2021,\n",
       "  3185,\n",
       "  3849,\n",
       "  2200,\n",
       "  4416,\n",
       "  1011,\n",
       "  1999,\n",
       "  1011,\n",
       "  5048,\n",
       "  1012,\n",
       "  2151,\n",
       "  8562,\n",
       "  2003,\n",
       "  2025,\n",
       "  1997,\n",
       "  1996,\n",
       "  6057,\n",
       "  2785,\n",
       "  1012,\n",
       "  2561,\n",
       "  2622,\n",
       "  3849,\n",
       "  2000,\n",
       "  2031,\n",
       "  1996,\n",
       "  3737,\n",
       "  1997,\n",
       "  1037,\n",
       "  4248,\n",
       "  2666,\n",
       "  1998,\n",
       "  2012,\n",
       "  2335,\n",
       "  5912,\n",
       "  2003,\n",
       "  2126,\n",
       "  2058,\n",
       "  1996,\n",
       "  2327,\n",
       "  1012,\n",
       "  2023,\n",
       "  3185,\n",
       "  2003,\n",
       "  2055,\n",
       "  1037,\n",
       "  5896,\n",
       "  2108,\n",
       "  2128,\n",
       "  15773,\n",
       "  2077,\n",
       "  2183,\n",
       "  2000,\n",
       "  1996,\n",
       "  3898,\n",
       "  1012,\n",
       "  1012,\n",
       "  1012,\n",
       "  2023,\n",
       "  2323,\n",
       "  2031,\n",
       "  3047,\n",
       "  2000,\n",
       "  2023,\n",
       "  5896,\n",
       "  1012,\n",
       "  102,\n",
       "  101,\n",
       "  2709,\n",
       "  2000,\n",
       "  6644,\n",
       "  2011,\n",
       "  1996,\n",
       "  2697,\n",
       "  2515,\n",
       "  2025,\n",
       "  1010,\n",
       "  1999,\n",
       "  2151,\n",
       "  2126,\n",
       "  1010,\n",
       "  3233,\n",
       "  2039,\n",
       "  2000,\n",
       "  1996,\n",
       "  2434,\n",
       "  1012,\n",
       "  2007,\n",
       "  2069,\n",
       "  2028,\n",
       "  2364,\n",
       "  2839,\n",
       "  1006,\n",
       "  6156,\n",
       "  1007,\n",
       "  4192,\n",
       "  2005,\n",
       "  1996,\n",
       "  7367,\n",
       "  26426,\n",
       "  1010,\n",
       "  1996,\n",
       "  2143,\n",
       "  2003,\n",
       "  2025,\n",
       "  2130,\n",
       "  4276,\n",
       "  1996,\n",
       "  1016,\n",
       "  2847,\n",
       "  1997,\n",
       "  2115,\n",
       "  2051,\n",
       "  1012,\n",
       "  1045,\n",
       "  2572,\n",
       "  1037,\n",
       "  4121,\n",
       "  5470,\n",
       "  1997,\n",
       "  1996,\n",
       "  2034,\n",
       "  2143,\n",
       "  1010,\n",
       "  1996,\n",
       "  2466,\n",
       "  2240,\n",
       "  1998,\n",
       "  3772,\n",
       "  2001,\n",
       "  2428,\n",
       "  2204,\n",
       "  1010,\n",
       "  2021,\n",
       "  2023,\n",
       "  2003,\n",
       "  2028,\n",
       "  3185,\n",
       "  2008,\n",
       "  1045,\n",
       "  2097,\n",
       "  2196,\n",
       "  2153,\n",
       "  3422,\n",
       "  1012,\n",
       "  2009,\n",
       "  2003,\n",
       "  10468,\n",
       "  5020,\n",
       "  2000,\n",
       "  2054,\n",
       "  1996,\n",
       "  7367,\n",
       "  26426,\n",
       "  2015,\n",
       "  2000,\n",
       "  3923,\n",
       "  9489,\n",
       "  1998,\n",
       "  10503,\n",
       "  6965,\n",
       "  2020,\n",
       "  2066,\n",
       "  1010,\n",
       "  2021,\n",
       "  2007,\n",
       "  2172,\n",
       "  4788,\n",
       "  3772,\n",
       "  1012,\n",
       "  1045,\n",
       "  1005,\n",
       "  2310,\n",
       "  7714,\n",
       "  2464,\n",
       "  2488,\n",
       "  3772,\n",
       "  1999,\n",
       "  7815,\n",
       "  14281,\n",
       "  1010,\n",
       "  2009,\n",
       "  2003,\n",
       "  2061,\n",
       "  6770,\n",
       "  18424,\n",
       "  2008,\n",
       "  2017,\n",
       "  2074,\n",
       "  2031,\n",
       "  2000,\n",
       "  4756,\n",
       "  1012,\n",
       "  1045,\n",
       "  1010,\n",
       "  1999,\n",
       "  2053,\n",
       "  2126,\n",
       "  1010,\n",
       "  16755,\n",
       "  2023,\n",
       "  3185,\n",
       "  2000,\n",
       "  3087,\n",
       "  1010,\n",
       "  3666,\n",
       "  2009,\n",
       "  2097,\n",
       "  2074,\n",
       "  20010,\n",
       "  22648,\n",
       "  2102,\n",
       "  2013,\n",
       "  1996,\n",
       "  2034,\n",
       "  1012,\n",
       "  102,\n",
       "  101,\n",
       "  1045,\n",
       "  2245,\n",
       "  2002,\n",
       "  5149,\n",
       "  22715,\n",
       "  7971,\n",
       "  1006,\n",
       "  2040,\n",
       "  3266,\n",
       "  2000,\n",
       "  4019,\n",
       "  2013,\n",
       "  1996,\n",
       "  28259,\n",
       "  6542,\n",
       "  1997,\n",
       "  2112,\n",
       "  2028,\n",
       "  1007,\n",
       "  2001,\n",
       "  2183,\n",
       "  2000,\n",
       "  2022,\n",
       "  1999,\n",
       "  2112,\n",
       "  1016,\n",
       "  3984,\n",
       "  2025,\n",
       "  1012,\n",
       "  1045,\n",
       "  2074,\n",
       "  2228,\n",
       "  2027,\n",
       "  2323,\n",
       "  1997,\n",
       "  2730,\n",
       "  2014,\n",
       "  2125,\n",
       "  2066,\n",
       "  1999,\n",
       "  5958,\n",
       "  1996,\n",
       "  6122,\n",
       "  2112,\n",
       "  1016,\n",
       "  1006,\n",
       "  2017,\n",
       "  2113,\n",
       "  2054,\n",
       "  1045,\n",
       "  2812,\n",
       "  1007,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2023,\n",
       "  3185,\n",
       "  2066,\n",
       "  6978,\n",
       "  1017,\n",
       "  1010,\n",
       "  1998,\n",
       "  3923,\n",
       "  5722,\n",
       "  1016,\n",
       "  2628,\n",
       "  5691,\n",
       "  2306,\n",
       "  1037,\n",
       "  3185,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2023,\n",
       "  2001,\n",
       "  5760,\n",
       "  10231,\n",
       "  999,\n",
       "  1996,\n",
       "  2878,\n",
       "  3185,\n",
       "  2306,\n",
       "  1037,\n",
       "  3185,\n",
       "  10231,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2919,\n",
       "  2994,\n",
       "  2185,\n",
       "  999,\n",
       "  102,\n",
       "  101,\n",
       "  7193,\n",
       "  12670,\n",
       "  12053,\n",
       "  2058,\n",
       "  2673,\n",
       "  2164,\n",
       "  1996,\n",
       "  2516,\n",
       "  5537,\n",
       "  1006,\n",
       "  2085,\n",
       "  2008,\n",
       "  2003,\n",
       "  6057,\n",
       "  1007,\n",
       "  2052,\n",
       "  2031,\n",
       "  2149,\n",
       "  2903,\n",
       "  2023,\n",
       "  2003,\n",
       "  2070,\n",
       "  4066,\n",
       "  1997,\n",
       "  21014,\n",
       "  9727,\n",
       "  1010,\n",
       "  2021,\n",
       "  1010,\n",
       "  3404,\n",
       "  2033,\n",
       "  12455,\n",
       "  1010,\n",
       "  2023,\n",
       "  2003,\n",
       "  2028,\n",
       "  1997,\n",
       "  1996,\n",
       "  2087,\n",
       "  16436,\n",
       "  2135,\n",
       "  2919,\n",
       "  3152,\n",
       "  2017,\n",
       "  2071,\n",
       "  2412,\n",
       "  2156,\n",
       "  1010,\n",
       "  1998,\n",
       "  2065,\n",
       "  2017,\n",
       "  1005,\n",
       "  2128,\n",
       "  2025,\n",
       "  5870,\n",
       "  2012,\n",
       "  2009,\n",
       "  2274,\n",
       "  2781,\n",
       "  1999,\n",
       "  1010,\n",
       "  1045,\n",
       "  1005,\n",
       "  1040,\n",
       "  2360,\n",
       "  2017,\n",
       "  1005,\n",
       "  2310,\n",
       "  2439,\n",
       "  2115,\n",
       "  3168,\n",
       "  1997,\n",
       "  8562,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2585,\n",
       "  9152,\n",
       "  8159,\n",
       "  3248,\n",
       "  1037,\n",
       "  20076,\n",
       "  1998,\n",
       "  11655,\n",
       "  3567,\n",
       "  3527,\n",
       "  1011,\n",
       "  2022,\n",
       "  6499,\n",
       "  16190,\n",
       "  7148,\n",
       "  4405,\n",
       "  2040,\n",
       "  5064,\n",
       "  6732,\n",
       "  2009,\n",
       "  6413,\n",
       "  2000,\n",
       "  8526,\n",
       "  2019,\n",
       "  8605,\n",
       "  3085,\n",
       "  1006,\n",
       "  2931,\n",
       "  1007,\n",
       "  2250,\n",
       "  4026,\n",
       "  11486,\n",
       "  1999,\n",
       "  2019,\n",
       "  6832,\n",
       "  4512,\n",
       "  2055,\n",
       "  2293,\n",
       "  1010,\n",
       "  2074,\n",
       "  2004,\n",
       "  2002,\n",
       "  1005,\n",
       "  1055,\n",
       "  29059,\n",
       "  2000,\n",
       "  2010,\n",
       "  3056,\n",
       "  1998,\n",
       "  15443,\n",
       "  2331,\n",
       "  1012,\n",
       "  1006,\n",
       "  3475,\n",
       "  1005,\n",
       "  1056,\n",
       "  2009,\n",
       "  6298,\n",
       "  1012,\n",
       "  1012,\n",
       "  1012,\n",
       "  1007,\n",
       "  1997,\n",
       "  2607,\n",
       "  1010,\n",
       "  2002,\n",
       "  1005,\n",
       "  1055,\n",
       "  16891,\n",
       "  2011,\n",
       "  1037,\n",
       "  21864,\n",
       "  8024,\n",
       "  1997,\n",
       "  29081,\n",
       "  3382,\n",
       "  1010,\n",
       "  1998,\n",
       "  9378,\n",
       "  2229,\n",
       "  2039,\n",
       "  2006,\n",
       "  1996,\n",
       "  3509,\n",
       "  1010,\n",
       "  2074,\n",
       "  2004,\n",
       "  2023,\n",
       "  2168,\n",
       "  2250,\n",
       "  4026,\n",
       "  11486,\n",
       "  2003,\n",
       "  5559,\n",
       "  2011,\n",
       "  2006,\n",
       "  2014,\n",
       "  10165,\n",
       "  1012,\n",
       "  1006,\n",
       "  2027,\n",
       "  3202,\n",
       "  29119,\n",
       "  1007,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2559,\n",
       "  2627,\n",
       "  1996,\n",
       "  13576,\n",
       "  24004,\n",
       "  1011,\n",
       "  14253,\n",
       "  4942,\n",
       "  18209,\n",
       "  2015,\n",
       "  1010,\n",
       "  1006,\n",
       "  2061,\n",
       "  2058,\n",
       "  1996,\n",
       "  2327,\n",
       "  2017,\n",
       "  2428,\n",
       "  2342,\n",
       "  2000,\n",
       "  6523,\n",
       "  2000,\n",
       "  2068,\n",
       "  2004,\n",
       "  3565,\n",
       "  18209,\n",
       "  2015,\n",
       "  1010,\n",
       "  2013,\n",
       "  1037,\n",
       "  6248,\n",
       "  2879,\n",
       "  3564,\n",
       "  6436,\n",
       "  1011,\n",
       "  25024,\n",
       "  2094,\n",
       "  1999,\n",
       "  1996,\n",
       "  5472,\n",
       "  2652,\n",
       "  1996,\n",
       "  3185,\n",
       "  1005,\n",
       "  1055,\n",
       "  13132,\n",
       "  1011,\n",
       "  4224,\n",
       "  1011,\n",
       "  25325,\n",
       "  10179,\n",
       "  2890,\n",
       "  4323,\n",
       "  2006,\n",
       "  2010,\n",
       "  2210,\n",
       "  8928,\n",
       "  1010,\n",
       "  2000,\n",
       "  1037,\n",
       "  17617,\n",
       "  18092,\n",
       "  2061,\n",
       "  3409,\n",
       "  2100,\n",
       "  1013,\n",
       "  3035,\n",
       "  1011,\n",
       "  1061,\n",
       "  2010,\n",
       "  5789,\n",
       "  2003,\n",
       "  9850,\n",
       "  2094,\n",
       "  2006,\n",
       "  2062,\n",
       "  12246,\n",
       "  2084,\n",
       "  1996,\n",
       "  6456,\n",
       "  1005,\n",
       "  1007,\n",
       "  1010,\n",
       "  1996,\n",
       "  2087,\n",
       "  13576,\n",
       "  5919,\n",
       "  1997,\n",
       "  1996,\n",
       "  3185,\n",
       "  2024,\n",
       "  2129,\n",
       "  2009,\n",
       "  25308,\n",
       "  2015,\n",
       "  2107,\n",
       "  2919,\n",
       "  2482,\n",
       "  5555,\n",
       "  22662,\n",
       "  1997,\n",
       "  2120,\n",
       "  1998,\n",
       "  5762,\n",
       "  22807,\n",
       "  2046,\n",
       "  1037,\n",
       "  9530,\n",
       "  6767,\n",
       "  7630,\n",
       "  3064,\n",
       "  3535,\n",
       "  2000,\n",
       "  7475,\n",
       "  2070,\n",
       "  2785,\n",
       "  1997,\n",
       "  2391,\n",
       "  2055,\n",
       "  1996,\n",
       "  5415,\n",
       "  3267,\n",
       "  1998,\n",
       "  2373,\n",
       "  1997,\n",
       "  2293,\n",
       "  1012,\n",
       "  2057,\n",
       "  2131,\n",
       "  2009,\n",
       "  1011,\n",
       "  1011,\n",
       "  4875,\n",
       "  3337,\n",
       "  2066,\n",
       "  3057,\n",
       "  1999,\n",
       "  18184,\n",
       "  1998,\n",
       "  8265,\n",
       "  1010,\n",
       "  1998,\n",
       "  3057,\n",
       "  2066,\n",
       "  1005,\n",
       "  7861,\n",
       "  2067,\n",
       "  1010,\n",
       "  1998,\n",
       "  1010,\n",
       "  4593,\n",
       "  1010,\n",
       "  2035,\n",
       "  2017,\n",
       "  2031,\n",
       "  2000,\n",
       "  2079,\n",
       "  2003,\n",
       "  5390,\n",
       "  1037,\n",
       "  2210,\n",
       "  2000,\n",
       "  2191,\n",
       "  2009,\n",
       "  7015,\n",
       "  2438,\n",
       "  2005,\n",
       "  2115,\n",
       "  3185,\n",
       "  2000,\n",
       "  2131,\n",
       "  2184,\n",
       "  3340,\n",
       "  2006,\n",
       "  10047,\n",
       "  18939,\n",
       "  1012,\n",
       "  1012,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2004,\n",
       "  2005,\n",
       "  1996,\n",
       "  3737,\n",
       "  1997,\n",
       "  1996,\n",
       "  2537,\n",
       "  1010,\n",
       "  1996,\n",
       "  13717,\n",
       "  1013,\n",
       "  9260,\n",
       "  2003,\n",
       "  3532,\n",
       "  2438,\n",
       "  2000,\n",
       "  19653,\n",
       "  13675,\n",
       "  23180,\n",
       "  1010,\n",
       "  1998,\n",
       "  1996,\n",
       "  7497,\n",
       "  2003,\n",
       "  1010,\n",
       "  3383,\n",
       "  1010,\n",
       "  2130,\n",
       "  4788,\n",
       "  2084,\n",
       "  2008,\n",
       "  1010,\n",
       "  2021,\n",
       "  2017,\n",
       "  6684,\n",
       "  2031,\n",
       "  2051,\n",
       "  2000,\n",
       "  5060,\n",
       "  2138,\n",
       "  1996,\n",
       "  5896,\n",
       "  2003,\n",
       "  2061,\n",
       "  2919,\n",
       "  1012,\n",
       "  2045,\n",
       "  2024,\n",
       "  2399,\n",
       "  2209,\n",
       "  2007,\n",
       "  6627,\n",
       "  8713,\n",
       "  12898,\n",
       "  2099,\n",
       "  1010,\n",
       "  1006,\n",
       "  3649,\n",
       "  5235,\n",
       "  2005,\n",
       "  6014,\n",
       "  2003,\n",
       "  1999,\n",
       "  2304,\n",
       "  1998,\n",
       "  2317,\n",
       "  2065,\n",
       "  2017,\n",
       "  2064,\n",
       "  3275,\n",
       "  2041,\n",
       "  1996,\n",
       "  3168,\n",
       "  1999,\n",
       "  2008,\n",
       "  1007,\n",
       "  1010,\n",
       "  1998,\n",
       "  18921,\n",
       "  7377,\n",
       "  3527,\n",
       "  9328,\n",
       "  1010,\n",
       "  1006,\n",
       "  2061,\n",
       "  6057,\n",
       "  2026,\n",
       "  3507,\n",
       "  4378,\n",
       "  2266,\n",
       "  2040,\n",
       "  2788,\n",
       "  2066,\n",
       "  5691,\n",
       "  2066,\n",
       "  2023,\n",
       "  2941,\n",
       "  25471,\n",
       "  1998,\n",
       "  4191,\n",
       "  2043,\n",
       "  2059,\n",
       "  1996,\n",
       "  9986,\n",
       "  1005,\n",
       "  1055,\n",
       "  9055,\n",
       "  2633,\n",
       "  3092,\n",
       "  2039,\n",
       "  1999,\n",
       "  1037,\n",
       "  15443,\n",
       "  12006,\n",
       "  1007,\n",
       "  1010,\n",
       "  1998,\n",
       "  13184,\n",
       "  1011,\n",
       "  4367,\n",
       "  1010,\n",
       "  1006,\n",
       "  2029,\n",
       "  2003,\n",
       "  4569,\n",
       "  15580,\n",
       "  2102,\n",
       "  1997,\n",
       "  2035,\n",
       "  2138,\n",
       "  1996,\n",
       "  2931,\n",
       "  2599,\n",
       "  2003,\n",
       "  2061,\n",
       "  3532,\n",
       "  2012,\n",
       "  3061,\n",
       "  2145,\n",
       "  2017,\n",
       "  2113,\n",
       "  1996,\n",
       "  2754,\n",
       "  2398,\n",
       "  2020,\n",
       "  19739,\n",
       "  20961,\n",
       "  9328,\n",
       "  2125,\n",
       "  4950,\n",
       "  1007,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1996,\n",
       "  2190,\n",
       "  7171,\n",
       "  2024,\n",
       "  1996,\n",
       "  2220,\n",
       "  3924,\n",
       "  2006,\n",
       "  1996,\n",
       "  3509,\n",
       "  1010,\n",
       "  2021,\n",
       "  1010,\n",
       "  2044,\n",
       "  2008,\n",
       "  1010,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2035,\n",
       "  19448,\n",
       "  1012,\n",
       "  1996,\n",
       "  1006,\n",
       "  3048,\n",
       "  2066,\n",
       "  2019,\n",
       "  9686,\n",
       "  25015,\n",
       "  4263,\n",
       "  2003,\n",
       "  3048,\n",
       "  1007,\n",
       "  10714,\n",
       "  2003,\n",
       "  6684,\n",
       "  1996,\n",
       "  19693,\n",
       "  4084,\n",
       "  1010,\n",
       "  2000,\n",
       "  2360,\n",
       "  1996,\n",
       "  2560,\n",
       "  1010,\n",
       "  1998,\n",
       "  1045,\n",
       "  1005,\n",
       "  1040,\n",
       "  2428,\n",
       "  14046,\n",
       "  3087,\n",
       "  2013,\n",
       "  3110,\n",
       "  2066,\n",
       "  2027,\n",
       "  1005,\n",
       "  1040,\n",
       "  2031,\n",
       "  2000,\n",
       "  2156,\n",
       "  2023,\n",
       "  20342,\n",
       "  3535,\n",
       "  2012,\n",
       "  3185,\n",
       "  1011,\n",
       "  2437,\n",
       "  2006,\n",
       "  2037,\n",
       "  4070,\n",
       "  1012,\n",
       "  1996,\n",
       "  3185,\n",
       "  3452,\n",
       "  2003,\n",
       "  2919,\n",
       "  2438,\n",
       "  2000,\n",
       "  2022,\n",
       "  6057,\n",
       "  1010,\n",
       "  1998,\n",
       "  2008,\n",
       "  1005,\n",
       "  1055,\n",
       "  2055,\n",
       "  1996,\n",
       "  ...],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...],\n",
       " 'word_ids': [None,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  None,\n",
       "  None,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  83,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  137,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  None,\n",
       "  None,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  None,\n",
       "  None,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  86,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  88,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  192,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  207,\n",
       "  207,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  229,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  249,\n",
       "  250,\n",
       "  251,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  272,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  280,\n",
       "  280,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  285,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300,\n",
       "  301,\n",
       "  302,\n",
       "  303,\n",
       "  304,\n",
       "  305,\n",
       "  306,\n",
       "  307,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  318,\n",
       "  319,\n",
       "  320,\n",
       "  321,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  357,\n",
       "  358,\n",
       "  359,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  364,\n",
       "  365,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  369,\n",
       "  370,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  400,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  405,\n",
       "  406,\n",
       "  406,\n",
       "  406,\n",
       "  406,\n",
       "  407,\n",
       "  408,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  421,\n",
       "  422,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  430,\n",
       "  430,\n",
       "  430,\n",
       "  431,\n",
       "  432,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  443,\n",
       "  444,\n",
       "  445,\n",
       "  446,\n",
       "  447,\n",
       "  448,\n",
       "  449,\n",
       "  450,\n",
       "  451,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  457,\n",
       "  458,\n",
       "  459,\n",
       "  460,\n",
       "  461,\n",
       "  462,\n",
       "  463,\n",
       "  464,\n",
       "  465,\n",
       "  466,\n",
       "  467,\n",
       "  468,\n",
       "  469,\n",
       "  470,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  473,\n",
       "  473,\n",
       "  474,\n",
       "  475,\n",
       "  476,\n",
       "  477,\n",
       "  478,\n",
       "  479,\n",
       "  480,\n",
       "  481,\n",
       "  482,\n",
       "  483,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  487,\n",
       "  488,\n",
       "  489,\n",
       "  490,\n",
       "  491,\n",
       "  492,\n",
       "  492,\n",
       "  492,\n",
       "  493,\n",
       "  494,\n",
       "  495,\n",
       "  496,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  500,\n",
       "  501,\n",
       "  502,\n",
       "  503,\n",
       "  504,\n",
       "  505,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  509,\n",
       "  510,\n",
       "  511,\n",
       "  512,\n",
       "  513,\n",
       "  514,\n",
       "  515,\n",
       "  516,\n",
       "  517,\n",
       "  518,\n",
       "  519,\n",
       "  520,\n",
       "  521,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  532,\n",
       "  532,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  536,\n",
       "  537,\n",
       "  538,\n",
       "  539,\n",
       "  540,\n",
       "  541,\n",
       "  542,\n",
       "  543,\n",
       "  544,\n",
       "  545,\n",
       "  546,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  554,\n",
       "  555,\n",
       "  556,\n",
       "  557,\n",
       "  558,\n",
       "  559,\n",
       "  560,\n",
       "  561,\n",
       "  562,\n",
       "  563,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  570,\n",
       "  571,\n",
       "  572,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  576,\n",
       "  577,\n",
       "  578,\n",
       "  579,\n",
       "  580,\n",
       "  581,\n",
       "  582,\n",
       "  583,\n",
       "  584,\n",
       "  585,\n",
       "  586,\n",
       "  587,\n",
       "  588,\n",
       "  589,\n",
       "  590,\n",
       "  ...]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'word_ids'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_examples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk length:128\n",
      "chunk length:128\n",
      "chunk length:128\n",
      "chunk length:128\n",
      "chunk length:128\n",
      "chunk length:128\n",
      "chunk length:128\n",
      "chunk length:128\n",
      "chunk length:128\n",
      "chunk length:92\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k:[t[i:i+chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks['input_ids']:\n",
    "    print(f\"chunk length:{len(chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5610472655384d52a88838f39f985962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb1dc36242a4a2783bfc6ef5b7a7fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285b892ca3654c6eb70d0e8722ea889d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 61291\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 59904\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 122957\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k:sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # split by chunks of max_len\n",
    "    result = {\n",
    "        k:[t[i:i+chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result['labels'] = result['input_ids'].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_datasets['train'][1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1045,\n",
       " 12524,\n",
       " 1045,\n",
       " 2572,\n",
       " 8025,\n",
       " 1011,\n",
       " 3756,\n",
       " 2013,\n",
       " 2026,\n",
       " 2678,\n",
       " 3573,\n",
       " 2138,\n",
       " 1997,\n",
       " 2035,\n",
       " 1996,\n",
       " 6704,\n",
       " 2008,\n",
       " 5129,\n",
       " 2009,\n",
       " 2043,\n",
       " 2009,\n",
       " 2001,\n",
       " 2034,\n",
       " 2207,\n",
       " 1999,\n",
       " 3476,\n",
       " 1012,\n",
       " 1045,\n",
       " 2036,\n",
       " 2657,\n",
       " 2008,\n",
       " 2012,\n",
       " 2034,\n",
       " 2009,\n",
       " 2001,\n",
       " 8243,\n",
       " 2011,\n",
       " 1057,\n",
       " 1012,\n",
       " 1055,\n",
       " 1012,\n",
       " 8205,\n",
       " 2065,\n",
       " 2009,\n",
       " 2412,\n",
       " 2699,\n",
       " 2000,\n",
       " 4607,\n",
       " 2023,\n",
       " 2406,\n",
       " 1010,\n",
       " 3568,\n",
       " 2108,\n",
       " 1037,\n",
       " 5470,\n",
       " 1997,\n",
       " 3152,\n",
       " 2641,\n",
       " 1000,\n",
       " 6801,\n",
       " 1000,\n",
       " 1045,\n",
       " 2428,\n",
       " 2018,\n",
       " 2000,\n",
       " 2156,\n",
       " 2023,\n",
       " 2005,\n",
       " 2870,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1996,\n",
       " 5436,\n",
       " 2003,\n",
       " 8857,\n",
       " 2105,\n",
       " 1037,\n",
       " 2402,\n",
       " 4467,\n",
       " 3689,\n",
       " 3076,\n",
       " 2315,\n",
       " 14229,\n",
       " 2040,\n",
       " 4122,\n",
       " 2000,\n",
       " 4553,\n",
       " 2673,\n",
       " 2016,\n",
       " 2064,\n",
       " 2055,\n",
       " 2166,\n",
       " 1012,\n",
       " 1999,\n",
       " 3327,\n",
       " 2016,\n",
       " 4122,\n",
       " 2000,\n",
       " 3579,\n",
       " 2014,\n",
       " 3086,\n",
       " 2015,\n",
       " 2000,\n",
       " 2437,\n",
       " 2070,\n",
       " 4066,\n",
       " 1997,\n",
       " 4516,\n",
       " 2006,\n",
       " 2054,\n",
       " 1996,\n",
       " 2779,\n",
       " 25430,\n",
       " 14728,\n",
       " 2245,\n",
       " 2055,\n",
       " 3056,\n",
       " 2576,\n",
       " 3314,\n",
       " 2107]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets['train'][0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2004,\n",
       " 1996,\n",
       " 5148,\n",
       " 2162,\n",
       " 1998,\n",
       " 2679,\n",
       " 3314,\n",
       " 1999,\n",
       " 1996,\n",
       " 2142,\n",
       " 2163,\n",
       " 1012,\n",
       " 1999,\n",
       " 2090,\n",
       " 4851,\n",
       " 8801,\n",
       " 1998,\n",
       " 6623,\n",
       " 7939,\n",
       " 4697,\n",
       " 3619,\n",
       " 1997,\n",
       " 8947,\n",
       " 2055,\n",
       " 2037,\n",
       " 10740,\n",
       " 2006,\n",
       " 4331,\n",
       " 1010,\n",
       " 2016,\n",
       " 2038,\n",
       " 3348,\n",
       " 2007,\n",
       " 2014,\n",
       " 3689,\n",
       " 3836,\n",
       " 1010,\n",
       " 19846,\n",
       " 1010,\n",
       " 1998,\n",
       " 2496,\n",
       " 2273,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 2054,\n",
       " 8563,\n",
       " 2033,\n",
       " 2055,\n",
       " 1045,\n",
       " 2572,\n",
       " 8025,\n",
       " 1011,\n",
       " 3756,\n",
       " 2003,\n",
       " 2008,\n",
       " 2871,\n",
       " 2086,\n",
       " 3283,\n",
       " 1010,\n",
       " 2023,\n",
       " 2001,\n",
       " 2641,\n",
       " 26932,\n",
       " 1012,\n",
       " 2428,\n",
       " 1010,\n",
       " 1996,\n",
       " 3348,\n",
       " 1998,\n",
       " 16371,\n",
       " 25469,\n",
       " 5019,\n",
       " 2024,\n",
       " 2261,\n",
       " 1998,\n",
       " 2521,\n",
       " 2090,\n",
       " 1010,\n",
       " 2130,\n",
       " 2059,\n",
       " 2009,\n",
       " 1005,\n",
       " 1055,\n",
       " 2025,\n",
       " 2915,\n",
       " 2066,\n",
       " 2070,\n",
       " 10036,\n",
       " 2135,\n",
       " 2081,\n",
       " 22555,\n",
       " 2080,\n",
       " 1012,\n",
       " 2096,\n",
       " 2026,\n",
       " 2406,\n",
       " 3549,\n",
       " 2568,\n",
       " 2424,\n",
       " 2009,\n",
       " 16880,\n",
       " 1010,\n",
       " 1999,\n",
       " 4507,\n",
       " 3348,\n",
       " 1998,\n",
       " 16371,\n",
       " 25469,\n",
       " 2024,\n",
       " 1037,\n",
       " 2350,\n",
       " 18785,\n",
       " 1999,\n",
       " 4467,\n",
       " 5988,\n",
       " 1012,\n",
       " 2130,\n",
       " 13749,\n",
       " 7849,\n",
       " 24544,\n",
       " 1010]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets['train'][1]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets['train'][1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets['train'][1]['labels'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finetuning with trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [101,\n",
       "   1045,\n",
       "   12524,\n",
       "   1045,\n",
       "   2572,\n",
       "   8025,\n",
       "   1011,\n",
       "   3756,\n",
       "   2013,\n",
       "   2026,\n",
       "   2678,\n",
       "   3573,\n",
       "   2138,\n",
       "   1997,\n",
       "   2035,\n",
       "   1996,\n",
       "   6704,\n",
       "   2008,\n",
       "   5129,\n",
       "   2009,\n",
       "   2043,\n",
       "   2009,\n",
       "   2001,\n",
       "   2034,\n",
       "   2207,\n",
       "   1999,\n",
       "   3476,\n",
       "   1012,\n",
       "   1045,\n",
       "   2036,\n",
       "   2657,\n",
       "   2008,\n",
       "   2012,\n",
       "   2034,\n",
       "   2009,\n",
       "   2001,\n",
       "   8243,\n",
       "   2011,\n",
       "   1057,\n",
       "   1012,\n",
       "   1055,\n",
       "   1012,\n",
       "   8205,\n",
       "   2065,\n",
       "   2009,\n",
       "   2412,\n",
       "   2699,\n",
       "   2000,\n",
       "   4607,\n",
       "   2023,\n",
       "   2406,\n",
       "   1010,\n",
       "   3568,\n",
       "   2108,\n",
       "   1037,\n",
       "   5470,\n",
       "   1997,\n",
       "   3152,\n",
       "   2641,\n",
       "   1000,\n",
       "   6801,\n",
       "   1000,\n",
       "   1045,\n",
       "   2428,\n",
       "   2018,\n",
       "   2000,\n",
       "   2156,\n",
       "   2023,\n",
       "   2005,\n",
       "   2870,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1996,\n",
       "   5436,\n",
       "   2003,\n",
       "   8857,\n",
       "   2105,\n",
       "   1037,\n",
       "   2402,\n",
       "   4467,\n",
       "   3689,\n",
       "   3076,\n",
       "   2315,\n",
       "   14229,\n",
       "   2040,\n",
       "   4122,\n",
       "   2000,\n",
       "   4553,\n",
       "   2673,\n",
       "   2016,\n",
       "   2064,\n",
       "   2055,\n",
       "   2166,\n",
       "   1012,\n",
       "   1999,\n",
       "   3327,\n",
       "   2016,\n",
       "   4122,\n",
       "   2000,\n",
       "   3579,\n",
       "   2014,\n",
       "   3086,\n",
       "   2015,\n",
       "   2000,\n",
       "   2437,\n",
       "   2070,\n",
       "   4066,\n",
       "   1997,\n",
       "   4516,\n",
       "   2006,\n",
       "   2054,\n",
       "   1996,\n",
       "   2779,\n",
       "   25430,\n",
       "   14728,\n",
       "   2245,\n",
       "   2055,\n",
       "   3056,\n",
       "   2576,\n",
       "   3314,\n",
       "   2107],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  'labels': [101,\n",
       "   1045,\n",
       "   12524,\n",
       "   1045,\n",
       "   2572,\n",
       "   8025,\n",
       "   1011,\n",
       "   3756,\n",
       "   2013,\n",
       "   2026,\n",
       "   2678,\n",
       "   3573,\n",
       "   2138,\n",
       "   1997,\n",
       "   2035,\n",
       "   1996,\n",
       "   6704,\n",
       "   2008,\n",
       "   5129,\n",
       "   2009,\n",
       "   2043,\n",
       "   2009,\n",
       "   2001,\n",
       "   2034,\n",
       "   2207,\n",
       "   1999,\n",
       "   3476,\n",
       "   1012,\n",
       "   1045,\n",
       "   2036,\n",
       "   2657,\n",
       "   2008,\n",
       "   2012,\n",
       "   2034,\n",
       "   2009,\n",
       "   2001,\n",
       "   8243,\n",
       "   2011,\n",
       "   1057,\n",
       "   1012,\n",
       "   1055,\n",
       "   1012,\n",
       "   8205,\n",
       "   2065,\n",
       "   2009,\n",
       "   2412,\n",
       "   2699,\n",
       "   2000,\n",
       "   4607,\n",
       "   2023,\n",
       "   2406,\n",
       "   1010,\n",
       "   3568,\n",
       "   2108,\n",
       "   1037,\n",
       "   5470,\n",
       "   1997,\n",
       "   3152,\n",
       "   2641,\n",
       "   1000,\n",
       "   6801,\n",
       "   1000,\n",
       "   1045,\n",
       "   2428,\n",
       "   2018,\n",
       "   2000,\n",
       "   2156,\n",
       "   2023,\n",
       "   2005,\n",
       "   2870,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1996,\n",
       "   5436,\n",
       "   2003,\n",
       "   8857,\n",
       "   2105,\n",
       "   1037,\n",
       "   2402,\n",
       "   4467,\n",
       "   3689,\n",
       "   3076,\n",
       "   2315,\n",
       "   14229,\n",
       "   2040,\n",
       "   4122,\n",
       "   2000,\n",
       "   4553,\n",
       "   2673,\n",
       "   2016,\n",
       "   2064,\n",
       "   2055,\n",
       "   2166,\n",
       "   1012,\n",
       "   1999,\n",
       "   3327,\n",
       "   2016,\n",
       "   4122,\n",
       "   2000,\n",
       "   3579,\n",
       "   2014,\n",
       "   3086,\n",
       "   2015,\n",
       "   2000,\n",
       "   2437,\n",
       "   2070,\n",
       "   4066,\n",
       "   1997,\n",
       "   4516,\n",
       "   2006,\n",
       "   2054,\n",
       "   1996,\n",
       "   2779,\n",
       "   25430,\n",
       "   14728,\n",
       "   2245,\n",
       "   2055,\n",
       "   3056,\n",
       "   2576,\n",
       "   3314,\n",
       "   2107]},\n",
       " {'input_ids': [2004,\n",
       "   1996,\n",
       "   5148,\n",
       "   2162,\n",
       "   1998,\n",
       "   2679,\n",
       "   3314,\n",
       "   1999,\n",
       "   1996,\n",
       "   2142,\n",
       "   2163,\n",
       "   1012,\n",
       "   1999,\n",
       "   2090,\n",
       "   4851,\n",
       "   8801,\n",
       "   1998,\n",
       "   6623,\n",
       "   7939,\n",
       "   4697,\n",
       "   3619,\n",
       "   1997,\n",
       "   8947,\n",
       "   2055,\n",
       "   2037,\n",
       "   10740,\n",
       "   2006,\n",
       "   4331,\n",
       "   1010,\n",
       "   2016,\n",
       "   2038,\n",
       "   3348,\n",
       "   2007,\n",
       "   2014,\n",
       "   3689,\n",
       "   3836,\n",
       "   1010,\n",
       "   19846,\n",
       "   1010,\n",
       "   1998,\n",
       "   2496,\n",
       "   2273,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   2054,\n",
       "   8563,\n",
       "   2033,\n",
       "   2055,\n",
       "   1045,\n",
       "   2572,\n",
       "   8025,\n",
       "   1011,\n",
       "   3756,\n",
       "   2003,\n",
       "   2008,\n",
       "   2871,\n",
       "   2086,\n",
       "   3283,\n",
       "   1010,\n",
       "   2023,\n",
       "   2001,\n",
       "   2641,\n",
       "   26932,\n",
       "   1012,\n",
       "   2428,\n",
       "   1010,\n",
       "   1996,\n",
       "   3348,\n",
       "   1998,\n",
       "   16371,\n",
       "   25469,\n",
       "   5019,\n",
       "   2024,\n",
       "   2261,\n",
       "   1998,\n",
       "   2521,\n",
       "   2090,\n",
       "   1010,\n",
       "   2130,\n",
       "   2059,\n",
       "   2009,\n",
       "   1005,\n",
       "   1055,\n",
       "   2025,\n",
       "   2915,\n",
       "   2066,\n",
       "   2070,\n",
       "   10036,\n",
       "   2135,\n",
       "   2081,\n",
       "   22555,\n",
       "   2080,\n",
       "   1012,\n",
       "   2096,\n",
       "   2026,\n",
       "   2406,\n",
       "   3549,\n",
       "   2568,\n",
       "   2424,\n",
       "   2009,\n",
       "   16880,\n",
       "   1010,\n",
       "   1999,\n",
       "   4507,\n",
       "   3348,\n",
       "   1998,\n",
       "   16371,\n",
       "   25469,\n",
       "   2024,\n",
       "   1037,\n",
       "   2350,\n",
       "   18785,\n",
       "   1999,\n",
       "   4467,\n",
       "   5988,\n",
       "   1012,\n",
       "   2130,\n",
       "   13749,\n",
       "   7849,\n",
       "   24544,\n",
       "   1010],\n",
       "  'attention_mask': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  'labels': [2004,\n",
       "   1996,\n",
       "   5148,\n",
       "   2162,\n",
       "   1998,\n",
       "   2679,\n",
       "   3314,\n",
       "   1999,\n",
       "   1996,\n",
       "   2142,\n",
       "   2163,\n",
       "   1012,\n",
       "   1999,\n",
       "   2090,\n",
       "   4851,\n",
       "   8801,\n",
       "   1998,\n",
       "   6623,\n",
       "   7939,\n",
       "   4697,\n",
       "   3619,\n",
       "   1997,\n",
       "   8947,\n",
       "   2055,\n",
       "   2037,\n",
       "   10740,\n",
       "   2006,\n",
       "   4331,\n",
       "   1010,\n",
       "   2016,\n",
       "   2038,\n",
       "   3348,\n",
       "   2007,\n",
       "   2014,\n",
       "   3689,\n",
       "   3836,\n",
       "   1010,\n",
       "   19846,\n",
       "   1010,\n",
       "   1998,\n",
       "   2496,\n",
       "   2273,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   2054,\n",
       "   8563,\n",
       "   2033,\n",
       "   2055,\n",
       "   1045,\n",
       "   2572,\n",
       "   8025,\n",
       "   1011,\n",
       "   3756,\n",
       "   2003,\n",
       "   2008,\n",
       "   2871,\n",
       "   2086,\n",
       "   3283,\n",
       "   1010,\n",
       "   2023,\n",
       "   2001,\n",
       "   2641,\n",
       "   26932,\n",
       "   1012,\n",
       "   2428,\n",
       "   1010,\n",
       "   1996,\n",
       "   3348,\n",
       "   1998,\n",
       "   16371,\n",
       "   25469,\n",
       "   5019,\n",
       "   2024,\n",
       "   2261,\n",
       "   1998,\n",
       "   2521,\n",
       "   2090,\n",
       "   1010,\n",
       "   2130,\n",
       "   2059,\n",
       "   2009,\n",
       "   1005,\n",
       "   1055,\n",
       "   2025,\n",
       "   2915,\n",
       "   2066,\n",
       "   2070,\n",
       "   10036,\n",
       "   2135,\n",
       "   2081,\n",
       "   22555,\n",
       "   2080,\n",
       "   1012,\n",
       "   2096,\n",
       "   2026,\n",
       "   2406,\n",
       "   3549,\n",
       "   2568,\n",
       "   2424,\n",
       "   2009,\n",
       "   16880,\n",
       "   1010,\n",
       "   1999,\n",
       "   4507,\n",
       "   3348,\n",
       "   1998,\n",
       "   16371,\n",
       "   25469,\n",
       "   2024,\n",
       "   1037,\n",
       "   2350,\n",
       "   18785,\n",
       "   1999,\n",
       "   4467,\n",
       "   5988,\n",
       "   1012,\n",
       "   2130,\n",
       "   13749,\n",
       "   7849,\n",
       "   24544,\n",
       "   1010]}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop('word_ids')\n",
    "\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i rented i am curious - yellow from my video [MASK] because of all the [MASK] that surrounded it when [MASK] was first released in 1967. i also heard that at first it was seized by u. s. customs if it [MASK] [MASK] [MASK] enter this country, therefore being [MASK] fan of films considered \" controversial \" i really had to see this for myself [MASK] [MASK] br / > [MASK] br / > the plot is centered around a young swedish drama student named lena [MASK] wants to yemen everything she can about life. in particular she wants to focus her attentions to [MASK] some sort stale documentary on what the average sw [MASK] thought about certain political issues such\n",
      "as the vietnam war and race issues in the united states. in between [MASK] politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama [MASK] [MASK] classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is [MASK] 40 years ago, this was [MASK] pornographic. really, the sex and magdity [MASK] are few [MASK] far [MASK], even then it's not shot like some cheaply made porno. while my countrymen [MASK] find it shocking, in [MASK] sex and nudity are a [MASK] [MASK] in swedish cinema. even ing [MASK] [MASK],\n"
     ]
    }
   ],
   "source": [
    "for chunk in data_collator(samples)['input_ids']: # token mask\n",
    "    print(f\"{tokenizer.decode(chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全词掩码 whole word mask\n",
    "import collections\n",
    "import numpy as np\n",
    "from transformers import default_data_collator\n",
    "wwm_probability = 0.2\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop('word_ids')\n",
    "\n",
    "        # create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None: # special tokens\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "\n",
    "        # randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature['input_ids']\n",
    "        labels = feature['labels']\n",
    "        new_labels = [-100] * len(labels) # labels are all -100 except for the ones corresponding to mask words.\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature['labels'] = new_labels\n",
    "\n",
    "    return default_data_collator(features)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] i [MASK] i am curious - yellow from my video store because [MASK] all the controversy that surrounded it when it was first released in 1967. i also [MASK] that at first it was [MASK] by [MASK]. s. customs if it ever [MASK] to enter this [MASK], therefore being [MASK] fan of [MASK] considered \" controversial \" i [MASK] had to see this for [MASK]. < br / > < br / > [MASK] plot is centered around a young swedish drama student named lena [MASK] wants to [MASK] everything she [MASK] [MASK] life. in particular she wants to [MASK] her attentions to [MASK] some sort of documentary on what the [MASK] swede thought about certain political issues such'\n",
      "\n",
      "'>>> [MASK] the vietnam [MASK] and race issues in the united [MASK]. in [MASK] [MASK] politicians [MASK] ordinary denizens of [MASK] about [MASK] opinions on politics, she has sex with her drama teacher, classmates, and married [MASK]. < br / > < br / [MASK] what kills me about i am curious - [MASK] [MASK] that 40 [MASK] ago, this was considered pornographic. really, the sex [MASK] nudity [MASK] are few and far between, even then it's not shot like some cheaply made porno [MASK] while [MASK] countrymen mind [MASK] it [MASK], [MASK] reality [MASK] and nudity are a major [MASK] in [MASK] cinema. even ingmar bergman,'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 10000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets['train'].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\transformers\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/repos/create (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000207523A7970>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))\"), '(Request ID: 5ecfd10c-afb0-4cd7-be2d-eb84dc1bc6f9)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dns_host, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mport), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\urllib3\\util\\connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m     97\u001b[0m \u001b[39mraise\u001b[39;00m socket\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mgetaddrinfo returns an empty list\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[0;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m sock\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\urllib3\\connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    404\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\urllib3\\connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1053\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\urllib3\\connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[0;32m    364\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x00000207523A7970>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\urllib3\\connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    796\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m--> 798\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    799\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    801\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/repos/create (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000207523A7970>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 21\u001b[0m\n\u001b[0;32m      5\u001b[0m model_name \u001b[39m=\u001b[39m model_checkpoint\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m      7\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      8\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mD:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mhuggingface\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m-finetuned-imdb\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m# no 中文路径\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     overwrite_output_dir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m )\n\u001b[1;32m---> 21\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     22\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     23\u001b[0m     args\u001b[39m=\u001b[39;49mtraining_args,\n\u001b[0;32m     24\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mdownsampled_dataset[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     25\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mdownsampled_dataset[\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     26\u001b[0m     data_collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[0;32m     27\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m     33\u001b[0m eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mevaluate()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\trainer.py:590\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhub_model_id \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    589\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpush_to_hub:\n\u001b[1;32m--> 590\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_hf_repo()\n\u001b[0;32m    591\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save:\n\u001b[0;32m    592\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39moutput_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\trainer.py:4135\u001b[0m, in \u001b[0;36mTrainer.init_hf_repo\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m   4132\u001b[0m     repo_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_model_id\n\u001b[0;32m   4134\u001b[0m token \u001b[39m=\u001b[39m token \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_token\n\u001b[1;32m-> 4135\u001b[0m repo_url \u001b[39m=\u001b[39m create_repo(repo_name, token\u001b[39m=\u001b[39;49mtoken, private\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhub_private_repo, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   4136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhub_model_id \u001b[39m=\u001b[39m repo_url\u001b[39m.\u001b[39mrepo_id\n\u001b[0;32m   4137\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpush_in_progress \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\huggingface_hub\\hf_api.py:3350\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[1;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[0;32m   3348\u001b[0m headers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_hf_headers(token\u001b[39m=\u001b[39mtoken)\n\u001b[0;32m   3349\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 3350\u001b[0m     r \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39;49mpost(path, headers\u001b[39m=\u001b[39;49mheaders, json\u001b[39m=\u001b[39;49mjson)\n\u001b[0;32m   3351\u001b[0m     \u001b[39mif\u001b[39;00m r\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m409\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCannot create repo: another conflicting operation is in progress\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39mtext:\n\u001b[0;32m   3352\u001b[0m         \u001b[39m# Since https://github.com/huggingface/moon-landing/pull/7272 (private repo), it is not possible to\u001b[39;00m\n\u001b[0;32m   3353\u001b[0m         \u001b[39m# concurrently create repos on the Hub for a same user. This is rarely an issue, except when running\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3356\u001b[0m         \u001b[39m# dependent libraries.\u001b[39;00m\n\u001b[0;32m   3357\u001b[0m         \u001b[39m# NOTE: If a fix is implemented server-side, we should be able to remove this retry mechanism.\u001b[39;00m\n\u001b[0;32m   3358\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreate repo failed due to a concurrency issue. Retrying...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\requests\\sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\u001b[39mself\u001b[39m, url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    627\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \n\u001b[0;32m    629\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(\u001b[39m\"\u001b[39m\u001b[39mPOST\u001b[39m\u001b[39m\"\u001b[39m, url, data\u001b[39m=\u001b[39mdata, json\u001b[39m=\u001b[39mjson, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:66\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[1;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     67\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     68\u001b[0m     request_id \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\transformers\\lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/repos/create (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000207523A7970>: Failed to establish a new connection: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。'))\"), '(Request ID: 5ecfd10c-afb0-4cd7-be2d-eb84dc1bc6f9)')"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(downsampled_dataset['train']) // batch_size\n",
    "model_name = model_checkpoint.split('/')[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'D:\\huggingface\\{model_name}-finetuned-imdb', # no 中文路径\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset['train'],\n",
    "    eval_dataset=downsampled_dataset['test'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\") # 21.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\") # 11.32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finetuning with accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "\n",
    "\n",
    "downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\n",
    "eval_dataset = downsampled_dataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset['train'],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, collate_fn=default_data_collator)\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[:len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(loss))\n",
    "    except OverflowError:\n",
    "        perplexity = float('inf')\n",
    "\n",
    "    print(f\"epoch {epoch}, perplexity:{perplexity}\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
