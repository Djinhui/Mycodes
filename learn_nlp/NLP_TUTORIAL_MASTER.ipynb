{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nlp-tutorial` is a tutorial for who is studying NLP(Natural Language Processing) using **Pytorch**. Most of the models in NLP were implemented with less than **100 lines** of code.(except comments or blank lines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Embedding Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. NNLM - Predict Next Word\n",
    "   - Paper -  [A Neural Probabilistic Language Model(2003)](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def make_batch():\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in sentences:\n",
    "        word = sen.split() # space tokenizer\n",
    "        input = [word_dict[n] for n in word[:-1]] # create (1~n-1) as input\n",
    "        target = word_dict[word[-1]] # create (n) as target, We usually call it 'casual language model'\n",
    "\n",
    "        input_batch.append(input)\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, target_batch\n",
    "\n",
    "\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNLM, self).__init__()\n",
    "        self.C = nn.Embedding(n_class, m)\n",
    "        self.H = nn.Linear(n_step*m, n_hidden, bias=False)\n",
    "        self.d = nn.Parameter(torch.ones(n_hidden))\n",
    "        self.U = nn.Linear(n_hidden, n_class, bias=False)\n",
    "        self.W = nn.Linear(n_step*m, n_class, bias=False)\n",
    "        self.b = nn.Parameter(torch.ones(n_class))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.C(X) # X:[batch_size, n_step, m]\n",
    "        X = X.view(-1, n_step*m) # [batch_size, n_step*m]\n",
    "        tanh = torch.tanh(self.d + self.H(X)) # [batch_size, n_hidden]\n",
    "        output = self.b + self.W(X) + self.U(tanh) # [batch_size, n_class]\n",
    "        return output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_step = 2 # number of steps , n-1 in paper\n",
    "    n_hidden = 2 # number of hidden size, h in paper\n",
    "    m = 2 # embedding size, m in paper\n",
    "\n",
    "    sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\"]\n",
    "\n",
    "    word_list = \" \".join(sentences).split()\n",
    "    word_list = list(set(word_list))\n",
    "    word_dict = {w:i for i, w in enumerate(word_list)}\n",
    "    number_dict = {i:w for i, w in enumerate(word_list)}\n",
    "    n_class = len(word_dict) # number of Vocabulary\n",
    "\n",
    "    model = NNLM()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, target_batch = make_batch()\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_batch)\n",
    "        # output:[batch_size, n_class], target_batch:[batch_size]\n",
    "        loss = criterion(output, target_batch)\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch+1), 'cost=', '{;.6f}'.format(loss))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "    print([sen.split()[:2] for sen in sentences],'->', [number_dict[n.item()] for n in predict.squeeze()])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Word2Vec-Skip-gram\n",
    "- Paper - [Distributed Representations of Words and Phrases\n",
    "    and their Compositionality(2013)](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def random_batch():\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False)\n",
    "    for i in random_index:\n",
    "        random_inputs.append(np.eye(voc_size)[skip_grams[i][0]]) # target\n",
    "        random_labels.append(skip_grams[i][1])\n",
    "\n",
    "    return random_inputs, random_labels\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        # W and WT is not transpose relationship\n",
    "        self.W = nn.Linear(voc_size, embedding_size, bias=False)\n",
    "        self.WT = nn.Linear(embedding_size, voc_size, bias=False)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # X:[batch_size, voc_size]\n",
    "        hidden_layer = self.W(X)\n",
    "        output_layer = self.WT(hidden_layer)\n",
    "        return output_layer # [batch_size, voc_size]\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    batch_size = 2\n",
    "    embedding_size = 2\n",
    "\n",
    "    sentences = [\"apple banana fruit\", \"banana orange fruit\", \"orange banana fruit\",\n",
    "                 \"dog cat animal\", \"cat monkey animal\", \"monkey dog animal\"]\n",
    "    \n",
    "    word_sequence = \" \".join(sentences).split()\n",
    "    word_list = \" \".join(sentences).split()\n",
    "    word_list = list(set(word_list))\n",
    "    word_dict = {w:i for i, w in enumerate(word_list)}\n",
    "    voc_size = len(word_list)\n",
    "\n",
    "    # make skip gram of one size window\n",
    "    skip_grams = []\n",
    "    for i in range(1, len(word_sequence) - 1):\n",
    "        target = word_dict[word_sequence[i]]\n",
    "        context = [word_dict[word_sequence[i-1]], word_dict[word_sequence[i+1]]]\n",
    "        for w in context:\n",
    "            skip_grams.append([target, w])\n",
    "\n",
    "    model = Word2Vec()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(5000):\n",
    "        input_batch, target_batch = random_batch()\n",
    "        input_batch = torch.Tensor(input_batch)\n",
    "        target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_batch)\n",
    "        # output : [batch_size, voc_size], target_batch : [batch_size] (LongTensor, not one-hot)\n",
    "        loss = criterion(output, target_batch)\n",
    "        if (epoch+1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch+1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    for i, label in enumerate(word_list):\n",
    "        W, WT = model.parameters() # W: torch.Size([embedding_size, voc_size])\n",
    "        x, y = W[0][i].item(), W[1][i].item()\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. FastText\n",
    "- Paper - [Bag of Tricks for Efficient Text Classification(2016)](https://arxiv.org/pdf/1607.01759.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/facebookresearch/fastText/archive/0.2.0.zip\n",
    "!unzip 0.2.0.zip\n",
    "%cd fastText-0.2.0\n",
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 is positive, 0 is negative\n",
    "f = open('train.txt', 'w')\n",
    "f.write('__label__1 i love you\\n')\n",
    "f.write('__label__1 he loves me\\n')\n",
    "f.write('__label__1 she likes baseball\\n')\n",
    "f.write('__label__0 i hate you\\n')\n",
    "f.write('__label__0 sorry for that\\n')\n",
    "f.write('__label__0 this is awful')\n",
    "f.close()\n",
    "\n",
    "f = open('test.txt', 'w')\n",
    "f.write('sorry hate you')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./fasttext supervised -input train.txt -output model -dim 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat test.txt\n",
    "!./fasttext predict model.bin test.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 TextCNN-Binary Sentiment Classification\n",
    "- Paper - [Convolutional Neural Networks for Sentence Classification(2014)](http://www.aclweb.org/anthology/D14-1181)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.W = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.Weight = nn.Linear(self.num_filters_total, num_classes, bias=False)\n",
    "        self.Bias = nn.Parameter(torch.ones([num_classes]))\n",
    "        self.filter_list = nn.ModuleList([nn.Conv2d(1, num_filters, (size, embedding_size)) for size in filter_sizes])\n",
    "\n",
    "    def forward(self, X):\n",
    "        embedded_chars = self.W(X) # [batch_size, sequence_lenth, embedding_size]\n",
    "        embedded_chars = embedded_chars.unsqueeze(1) # add channel(=1) [batch, channel(=1), sequence_length, embedding_size]\n",
    "\n",
    "        pooled_outputs = []\n",
    "        for i, conv in enumerate(self.filter_list):\n",
    "            h = F.relu(conv(embedded_chars))\n",
    "            mp = nn.MaxPool2d((sequence_length - filter_sizes[i]+1, 1))\n",
    "            pooled = mp(h).permute(0,3,2,1)\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "        h_pool = torch.cat(pooled_outputs, len(filter_sizes))\n",
    "        h_pool_flat = torch.reshape(h_pool, [-1, self.num_filters_total])\n",
    "        output = self.Weight(h_pool_flat) + self.Bias\n",
    "        return output\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    embedding_size = 2\n",
    "    sequence_length = 3\n",
    "    num_classes = 2\n",
    "    filter_sizes = [2,2,2]\n",
    "    num_filters = 3 \n",
    "\n",
    "    # 3 words sentences (=sequence_length is 3)\n",
    "    sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
    "    labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n",
    "\n",
    "    word_list = \" \".join(sentences).split()\n",
    "    word_list = list(set(word_list))\n",
    "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "    vocab_size = len(word_dict)\n",
    "\n",
    "    model = TextCNN()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    inputs = torch.LongTensor([np.asarray([word_dict[n] for n in sen.split()]) for sen in sentences])\n",
    "    targets = torch.LongTensor([out for out in labels])\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch+1), 'cost=', '{:.6f}'.format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test_text = 'sorry hate you'\n",
    "    tests = [np.asarray([word_dict[n] for n in test_text.split()])]\n",
    "    test_batch = torch.LongTensor(tests)\n",
    "    predict = model(test_batch).data.max(1, keepdim=True)[1]\n",
    "    if predict[0][0] == 0:\n",
    "        print(test_text, \"is Bad Mean...\")\n",
    "    else:\n",
    "        print(test_text,\"is Good Mean...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1 TextRNN -Predict Next Word\n",
    "- Paper - [Finding Structure in Time(1990)](http://psych.colorado.edu/~kimlab/Elman1990.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def make_batch():\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in sentences:\n",
    "        word = sen.split()\n",
    "        input = [word_dict[n] for n in word[:-1]]\n",
    "        target = word_dict[word[-1]]\n",
    "        input_batch.append(np.eye(n_class)[input]) # here is one-hot encoding\n",
    "        target_batch.append(target)\n",
    "    return input_batch, target_batch\n",
    "\n",
    "\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden)\n",
    "        self.W = nn.Linear(n_hidden, n_class, bias=False)\n",
    "        self.b = nn.Parameter(torch.ones([n_class]))\n",
    "    \n",
    "    def forward(self, hidden, X):\n",
    "        X = X.transpose(0, 1) # [n_step, batch_size, n_class] here is one-hot encoding\n",
    "        outputs, hidden = self.rnn(X, hidden)\n",
    "        # outputs : [n_step, batch_size, num_directions(=1) * n_hidden]\n",
    "        # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        outputs = outputs[-1] # last step [ batch_size, num_directions(=1) * n_hidden]\n",
    "        outputs = self.W(outputs) + self.b\n",
    "        return outputs\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    n_step = 2\n",
    "    n_hidden = 5\n",
    "    sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\"]\n",
    "\n",
    "    word_list = \" \".join(sentences).split()\n",
    "    word_list = list(set(word_list))\n",
    "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "    number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "    n_class = len(word_dict)\n",
    "    batch_size = len(sentences)\n",
    "\n",
    "    model = TextRNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    input_batch, target_batch = make_batch()\n",
    "    input_batch = torch.FloatTensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        optimizer.zero_grad()\n",
    "        hidden = torch.zeros(1, batch_size, n_hidden)\n",
    "        output = model(hidden, input_batch)\n",
    "        loss = criterion(output, target_batch)\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch+1), 'cost=', '{:.3f}'.format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    input = [sen.split()[:2] for sen in sentences]\n",
    "    hidden = torch.zeros(1, batch_size, n_hidden)\n",
    "    predict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\n",
    "    print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2 TextLSTM-Predict Next Word\n",
    "- Paper - [LONG SHORT-TERM MEMORY(1997)](https://www.bioinf.jku.at/publications/older/2604.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def make_batch():\n",
    "    input_batch, target_batch = [], []\n",
    "    for seq in seq_data:\n",
    "        input = [word_dict[n] for n in seq[:-1]]\n",
    "        target = word_dict[seq[-1]]\n",
    "        input_batch.append(np.eye(n_class)[input]) # onehot\n",
    "        target_batch.append(target)\n",
    "    return input_batch, target_batch\n",
    "\n",
    "class TextLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden)\n",
    "        self.W = nn.Linear(n_hidden, n_class, bias=False)\n",
    "        self.b = nn.Parameter(torch.ones([n_class]))\n",
    "\n",
    "    def forward(self, X):\n",
    "        input = X.transpose(0,1)\n",
    "        hidden_state = torch.zeros(1, len(X), n_hidden) # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        cell_state = torch.zeros(1, len(X), n_hidden)\n",
    "\n",
    "        outputs, (_,_) = self.lstm(input, (hidden_state, cell_state))\n",
    "        outputs = outputs[-1]\n",
    "        outputs = self.W(outputs) + self.b\n",
    "        return outputs\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    n_step = 3 # number of cells\n",
    "    n_hidden = 128 # number of hidden units in one cell\n",
    "\n",
    "    char_arr = [c for c in 'abcdefghijklmnopqrstuvwxyz']\n",
    "    word_dict = {n:i for i, n in enumerate(char_arr)}\n",
    "    number_dict = {i:w for i, w in enumerate(char_arr)}\n",
    "    n_class = len(word_dict)\n",
    "\n",
    "    seq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']\n",
    "    model = TextLSTM()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, target_batch = make_batch()\n",
    "    input_batch = torch.FloatTensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input_batch)\n",
    "        loss = criterion(output, target_batch)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    inputs = [sen[:3] for sen in seq_data]\n",
    "\n",
    "    predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "    print(inputs, '->', [number_dict[n.item()] for n in predict.squeeze()])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3 Bi-LSTM-Predict Next Word in Long Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def make_batch():\n",
    "    input_batch, target_batch = [], []\n",
    "    words = sentence.split()\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        input = [word_dict[n] for n in words[:(i+1)]]\n",
    "        input = input + [0] *(max_len - len(input))\n",
    "        target = word_dict[words[i+1]]\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        target_batch.append(target)\n",
    "    return input_batch, target_batch\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden,bidirectional=True)\n",
    "        self.W = nn.Linear(n_hidden*2, n_class, bias=False)\n",
    "        self.b = nn.Parameter(torch.ones([n_class]))\n",
    "\n",
    "    def forward(self, X):\n",
    "        input = X.transpose(0,1)\n",
    "        hidden_state = torch.zeros(1*2, len(X), n_hidden) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        cell_state = torch.zeros(1*2, len(X), n_hidden) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        \n",
    "        outputs, (_, _) = self.lstm(input, (hidden_state, cell_state))\n",
    "        outputs = outputs[-1]\n",
    "        outputs = self.W(outputs) + self.b\n",
    "        return outputs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_hidden = 5\n",
    "    sentence = (\n",
    "        'Lorem ipsum dolor sit amet consectetur adipisicing elit '\n",
    "        'sed do eiusmod tempor incididunt ut labore et dolore magna '\n",
    "        'aliqua Ut enim ad minim veniam quis nostrud exercitation'\n",
    "    )\n",
    "\n",
    "    word_dict = {w:i for i, w in enumerate(list(set(sentence.split())))}\n",
    "    number_dict = {i:w for i, w in enumerate(list(set(sentence.split())))}\n",
    "    n_class = len(word_dict)\n",
    "    max_len = len(sentence.split())\n",
    "\n",
    "    model = BiLSTM()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, target_batch = make_batch()\n",
    "    input_batch = torch.FloatTensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(10000):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_batch)\n",
    "        loss = criterion(output, target_batch)\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "    print(sentence)\n",
    "    print([number_dict[n.item()] for n in predict.squeeze()])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Attention Mechanism"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1 Seq2Seq Change Word\n",
    "- Paper - [Learning Phrase Representations using RNN Encoderâ€“Decoder\n",
    "    for Statistical Machine Translation(2014)](https://arxiv.org/pdf/1406.1078.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "\n",
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        input = [num_dic[n] for n in seq[0]]\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        output_batch.append(np.eye(n_class)[output])\n",
    "        target_batch.append(target) # not one-hot\n",
    "\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "def make_testbatch(input_word:str='man'):\n",
    "    input_batch, output_batch = [],[]\n",
    "    input_w = input_word + 'P' *(n_step - len(input_word))\n",
    "    input = [num_dic[n] for n in input_w]\n",
    "    output = [num_dic[n] for n in 'S' + 'P'*n_step]\n",
    "\n",
    "    input_batch = np.eye(n_class)[input]\n",
    "    output_batch = np.eye(n_class)[output]\n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        enc_input = enc_input.transpose(0,1) # [seq_len, batch_size, n_class]\n",
    "        dec_input = dec_input.transpose(0,1) # [seq_len, batch_size, n_class]\n",
    "\n",
    "        # enc_states:[num_layers(=1)*num_directions(=1), batch_size, n_hidden]\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
    "        # outputs:[seq_len, batch_size, num_directions*n_hidden]\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
    "        outputs = self.fc(outputs) # [seq_len, batch_size, n_class]\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    n_step = 5\n",
    "    n_hidden = 128\n",
    "\n",
    "    char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "    num_dic = {n:i for i, n in enumerate(char_arr)}\n",
    "    seq_data = [['man','woman'], ['black','white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down']]\n",
    "\n",
    "    n_class = len(num_dic)\n",
    "    batch_size = len(seq_data)\n",
    "\n",
    "    model = Seq2Seq()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch()\n",
    "    for epoch in range(5000):\n",
    "        hidden = torch.zeros(1, batch_size, n_hidden)\n",
    "        optimizer.zero_grad()\n",
    "        # input_batch : [batch_size, max_len(=n_step, time step), n_class]\n",
    "        # output_batch : [batch_size, max_len+1(=n_step, time step) (becase of 'S' or 'E'), n_class]\n",
    "        # target_batch : [batch_size, max_len+1(=n_step, time step)], not one-hot\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        output = output.transpose(0,1) # [batch_size, seq_len, n_class]\n",
    "        loss = 0\n",
    "        for i in range(0, len(target_batch)):\n",
    "            # output[i] : [max_len+1, n_class], target_batch[i] : [max_len+1]\n",
    "            loss += criterion(output[i], target_batch[i])\n",
    "        if (epoch+1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' %(epoch+1), 'cost =', '{:.3f}'.format(loss))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def translate(word):\n",
    "        input_batch, output_batch = make_testbatch(word)\n",
    "        hidden = torch.zeros(1,1,n_hidden)\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        predict = output.data.max(2, keepdim=True)[1]\n",
    "        decoded = [char_arr[i] for i in predict]\n",
    "        end = decoded.index('E')\n",
    "        translated = ''.join(decoded[:end])\n",
    "        return translated.replace('P','')\n",
    "    \n",
    "    print('test')\n",
    "    print('man ->', translate('man'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2 Seq2Seq with Attention - Translate\n",
    "- Paper - [Neural Machine Translation by Jointly Learning to Align and Translate(2014)](https://arxiv.org/abs/1409.0473)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "\n",
    "def make_batch():\n",
    "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
    "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
    "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(2*n_hidden, n_class)\n",
    "\n",
    "    def forward(self, enc_inputs,hidden, dec_inputs):\n",
    "        enc_inputs = enc_inputs.transpose(0,1) # [seq_len, batch_size, n_class]\n",
    "        dec_inputs = dec_inputs.transpose(0,1) # [seq_len, batch_size, n_class]\n",
    "\n",
    "        # enc_outputs:[seq_len, batch_size,num_directions(=1)*n_hidden]\n",
    "        # enc_hidden:[num_layers*num_directions, batch_size, n_hidden]\n",
    "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
    "\n",
    "        trained_attn = []\n",
    "        hidden = enc_hidden\n",
    "        n_step = len(dec_inputs)\n",
    "        outputs = torch.empty([n_step,1,n_class])\n",
    "        for i in range(n_step): # each time step\n",
    "            # dec_output:[seq_len(=1),batch_size=1,num_directions*n_hidden]\n",
    "            # hidden:[num_layer*num_directions, batch_size=1,n_hidden]\n",
    "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
    "            attn_weights = self.get_att_weight(dec_output, enc_outputs) # # attn_weights : [1, 1, n_step_enc]\n",
    "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
    "            \n",
    "            # matrix-matrix product of matrices [1,1,n_step_enc] x [1,n_step_enc,n_hidden] = [1,1,n_hidden]\n",
    "            context = attn_weights.bmm(enc_outputs.transpose(0,1))\n",
    "            dec_output = dec_output.squeeze(0)\n",
    "            context = context.squeeze(1)\n",
    "            outputs[i] = self.out(torch.cat((dec_output, context), 1))\n",
    "        return outputs.transpose(0,1).squeeze(0), trained_attn\n",
    "    \n",
    "    def get_att_weight(self, dec_output, enc_outputs):\n",
    "        # get attention weight one 'dec_output' with 'enc_outputs'\n",
    "        n_step = len(enc_outputs)\n",
    "        attn_scores = torch.zeros(n_step)\n",
    "        for i in range(n_step):\n",
    "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
    "        return F.softmax(attn_scores).view(1,1,-1)\n",
    "\n",
    "    def get_att_score(self, dec_output, enc_output):\n",
    "        # dec_output [1, 1, n_hidden]\n",
    "        # enc_output [batch_size=1, num_directions(=1) * n_hidden]\n",
    "        score = self.attn(enc_output)\n",
    "        return torch.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    n_step = 5\n",
    "    n_hidden = 128\n",
    "\n",
    "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "    word_list = \" \".join(sentences).split()\n",
    "    word_list = list(set(word_list))\n",
    "    word_dict = {w:i for i, w in enumerate(word_list)}\n",
    "    number_dict = {i:w for i, w in enumerate(word_list)}\n",
    "    n_class = len(word_dict)\n",
    "\n",
    "    hidden = torch.zeros(1, 1, n_hidden)\n",
    "    model = Attention()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(2000):\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(input_batch, hidden, output_batch)\n",
    "\n",
    "        loss = criterion(output, target_batch.squeeze(0))\n",
    "        if (epoch + 1) % 400 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test\n",
    "    test_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n",
    "    test_batch = torch.FloatTensor(test_batch)\n",
    "    predict, trained_attn = model(input_batch, hidden, test_batch)\n",
    "    predict = predict.data.max(1, keepdim=True)[1]\n",
    "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
    "\n",
    "    # Show Attention\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(trained_attn, cmap='viridis')\n",
    "    ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
    "    ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-3 BiLSTM with Attention -Binary Sentiment Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM_Attention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, n_hidden, bidirectional=True)\n",
    "        self.out = nn.Linear(2*n_hidden, num_classes)\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        # lstm_output:[batch_size, n_step, n_hidden * num_directions(=2)]\n",
    "        # final_state: [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        hidden = final_state.view(-1, n_hidden*2, 1) # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, n_step]\n",
    "        soft_attn_weights = F.softmax(attn_weights, dim=1)\n",
    "        context = torch.bmm(lstm_output.transpose(1,2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return context, soft_attn_weights.data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "\n",
    "    def forward(self, X):\n",
    "        input = self.embedding(X)\n",
    "        input = input.permute(1,0,2) # [seq_len, batch_size, embedding_dim]\n",
    "        hidden_state = torch.zeros(1*2, len(X), n_hidden)\n",
    "        cell_state = torch.zeros(1*2, len(X), n_hidden)\n",
    "\n",
    "        # final_hidden_state, final_cell_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "        output = output.permute(1,0,2) # [batch_size, seq_len, n_hidden*2]\n",
    "        attn_output, attention = self.attention_net(output, final_hidden_state)\n",
    "        return self.out(attn_output), attention  # model output : [batch_size, num_classes], attention : [batch_size, n_step]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    embedding_dim = 2\n",
    "    n_hidden = 5\n",
    "    num_classes = 2\n",
    "\n",
    "    # 3 words sentences (=sequence_length is 3)\n",
    "    sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
    "    labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n",
    "\n",
    "    word_list = ' '.join(sentences).split()\n",
    "    word_list = list(set(word_list))\n",
    "    word_dict = {w:i for i, w in enumerate(word_list)}\n",
    "    vocab_size = len(word_dict)\n",
    "\n",
    "    inputs = torch.LongTensor([np.asarray([word_dict[n] for n in sen.split()]) for sen in sentences])\n",
    "    targets = torch.LongTensor([out for out in labels])\n",
    "\n",
    "    model = BiLSTM_Attention()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(5000):\n",
    "        optimizer.zero_grad()\n",
    "        output, attention = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test\n",
    "    test_text = 'sorry hate you'\n",
    "    tests = [np.asarray([word_dict[n] for n in test_text.split()])]\n",
    "    test_batch = torch.LongTensor(tests)\n",
    "\n",
    "    # Predict\n",
    "    predict, _ = model(test_batch)\n",
    "    predict = predict.data.max(1, keepdim=True)[1]\n",
    "    if predict[0][0] == 0:\n",
    "        print(test_text,\"is Bad Mean...\")\n",
    "    else:\n",
    "        print(test_text,\"is Good Mean!!\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 3)) # [batch_size, n_step]\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    ax.set_xticklabels(['']+['first_word', 'second_word', 'third_word'], fontdict={'fontsize': 14}, rotation=90)\n",
    "    ax.set_yticklabels(['']+['batch_1', 'batch_2', 'batch_3', 'batch_4', 'batch_5', 'batch_6'], fontdict={'fontsize': 14})\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model based on Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-1 Transformer - Translate\n",
    "- Paper - [Attention Is All You Need(2017)](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "\n",
    "def make_batch(sentences):\n",
    "    input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n",
    "    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n",
    "    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]\n",
    "    return torch.LongTensor(input_batch), torch.LongTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position,d_model):\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2*(hid_idx // 2) / d_model)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return torch.FloatTensor(sinusoid_table)\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)\n",
    "\n",
    "def get_attn_subsequent_mask(seq):\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
    "    return subsequent_mask\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores = torch.matmul(Q, K.transpose(-1,-2)) / np.sqrt(d_k) \n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k*n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k*n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v*n_heads)\n",
    "        self.linear = nn.Linear(d_v*n_heads, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2) # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2) # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2) # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "        \n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "        # context: [batch_size x n_heads x len_q x d_v], \n",
    "        # attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1,2).contiguous().view(batch_size, -1, n_heads*d_v)\n",
    "        output = self.linear(context)\n",
    "        return self.layer_norm(output + residual), attn\n",
    "    \n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet,self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        residual = inputs\n",
    "        output = nn.ReLU()(self.conv1(inputs.transpose(1,2)))\n",
    "        output = self.conv2(output).transpose(1,2)\n",
    "        return self.layer_norm(residual + output)\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs, attn\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab_size,d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_len+1,d_model), freeze=True)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs):\n",
    "        # enc_inputs:[batch_size, src_len]\n",
    "        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        return enc_outputs, enc_self_attns\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs,enc_outputs,dec_self_attn_mask,dec_enc_attn_mask):\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs,dec_inputs,dec_inputs, dec_self_attn_mask)\n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_outputs = self.pos_ffn(dec_outputs)\n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(tgt_len+1, d_model))\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(torch.LongTensor([[5,1,2,3,4]]))\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs) # [batch_size, len_dec, len_dec]\n",
    "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs) # [batch_size, len_dec, len_dec]\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask+dec_self_attn_subsequent_mask), 0)\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
    "    \n",
    "        \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=F)\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        dec_logits = self.projection(dec_outputs)\n",
    "        return dec_logits.view(-1,dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n",
    "    \n",
    "\n",
    "def greedy_decoder(model, enc_input, start_symbol):\n",
    "    \"\"\"\n",
    "    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don't know the\n",
    "    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.\n",
    "    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n",
    "    :param model: Transformer Model\n",
    "    :param enc_input: The encoder input\n",
    "    :param start_symbol: The start symbol. In this example it is 'S' which corresponds to index 4\n",
    "    :return: The target input\n",
    "    \"\"\"\n",
    "    enc_outputs, enc_self_attns = model.encoder(enc_input)\n",
    "    dec_input = torch.zeros(1, 5).type_as(enc_input.data)\n",
    "    next_symbol = start_symbol\n",
    "    for i in range(0, 5):\n",
    "        dec_input[0][i] = next_symbol\n",
    "        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n",
    "        projected = model.projection(dec_outputs)\n",
    "        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
    "        next_word = prob.data[i]\n",
    "        next_symbol = next_word.item()\n",
    "    return dec_input\n",
    "\n",
    "\n",
    "def showgraph(attn):\n",
    "    attn = attn[-1].squeeze(0)[0]\n",
    "    attn = attn.squeeze(0).data.numpy()\n",
    "    fig = plt.figure(figsize=(n_heads, n_heads)) # [n_heads, n_heads]\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attn, cmap='viridis')\n",
    "    ax.set_xticklabels(['']+sentences[0].split(), fontdict={'fontsize': 14}, rotation=90)\n",
    "    ax.set_yticklabels(['']+sentences[2].split(), fontdict={'fontsize': 14})\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "    src_vocab = {'P':0, 'ich':1, 'mochte':2, 'ein':3, 'bier':4}\n",
    "    src_vocab_size = len(src_vocab)\n",
    "    tgt_vocab = {'P': 0, 'i': 1, 'want': 2, 'a': 3, 'beer': 4, 'S': 5, 'E': 6}\n",
    "    number_dict = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "    tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "    enc_inputs, dec_inputs, target_batch = make_batch(sentences)\n",
    "\n",
    "    src_len = 5\n",
    "    tgt_len = 5\n",
    "    d_model = 512\n",
    "    d_ff = 2048\n",
    "    d_k = d_v = 64\n",
    "    n_layers = 6\n",
    "    n_heads = 8\n",
    "\n",
    "    model = Transformer()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    enc_inputs, dec_inputs, target_batch = make_batch(sentences)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        optimizer.zero_grad()\n",
    "        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "        loss = criterion(outputs, target_batch.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    predict, _,_,_ = model(enc_inputs, dec_inputs)\n",
    "    predict = predict.data.max(1, keepdim=True)\n",
    "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
    "\n",
    "    print('first head of last state enc_self_attns')\n",
    "    showgraph(enc_self_attns)\n",
    "\n",
    "    print('first head of last state dec_self_attns')\n",
    "    showgraph(dec_self_attns)\n",
    "\n",
    "    print('first head of last state dec_enc_attns')\n",
    "    showgraph(dec_enc_attns)\n",
    "\n",
    "\n",
    "    greedy_dec_input = greedy_decoder(model, enc_inputs, start_symbol=tgt_vocab[\"S\"])\n",
    "    predict, _, _, _ = model(enc_inputs, greedy_dec_input)\n",
    "    predict = predict.data.max(1, keepdim=True)[1]\n",
    "    print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-2 BERT - Classification Next Sentence & Predict Maksed Tokens\n",
    "- Paper - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018)](https://arxiv.org/abs/1810.04805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# sample IsNext and NotNext to be same in small batch size\n",
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # MLM\n",
    "        n_pred = min(max_pred, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        cand_masked_pos = [i for i, token in enumerate(input_ids) if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "        shuffle(cand_masked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.8:\n",
    "                input_ids[pos] = word_dict['[MASK]']\n",
    "            elif random() < 0.5:\n",
    "                index = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = word_dict[num_dic[index]]\n",
    "\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0]*n_pad)\n",
    "        segment_ids.extend([0]*n_pad)\n",
    "\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pred)\n",
    "            masked_pos.extend([0] * n_pred)\n",
    "        \n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "\n",
    "    return batch\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k:torch.Tensor):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)\n",
    "\n",
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x) # seq_len --> (batch_size, seq_len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)\n",
    "    \n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(gelu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn\n",
    "    \n",
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layrers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=F)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layrers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        h_pooled = self.activ1(self.fc(output[:, 0])) # [cls]\n",
    "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "        # get masked position from final output of transformer.\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_clsf\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    maxlen = 30\n",
    "    batch_size = 6\n",
    "    max_pred = 5\n",
    "    n_layers = 6\n",
    "    n_heads = 12\n",
    "    d_model = 768\n",
    "    d_ff = 768*4\n",
    "    d_k = d_v = 64\n",
    "    n_segments = 2\n",
    "\n",
    "    text = (\n",
    "        'Hello, how are you? I am Romeo.\\n'\n",
    "        'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "        'Nice meet you too. How are you today?\\n'\n",
    "        'Great. My baseball team won the competition.\\n'\n",
    "        'Oh Congratulations, Juliet\\n'\n",
    "        'Thanks you Romeo'\n",
    "    )\n",
    "    sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')\n",
    "    word_list = list(set(\" \".join(sentences).split()))\n",
    "    word_dict = {'[PAD]':0, '[CLS]':1, '[SEP]':2, '[MASK]':3}\n",
    "    for i, w in enumerate(word_list):\n",
    "        word_dict[w] = i + 4\n",
    "    number_dict = {i:w for i, w in enumerate(word_dict)}\n",
    "    vocab_size = len(word_dict)\n",
    "\n",
    "    token_list = list()\n",
    "    for sentence in sentences:\n",
    "        arr = [word_dict[s] for s in sentence.split()]\n",
    "        token_list.append(arr)\n",
    "\n",
    "    batch = make_batch()\n",
    "    input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "    model = BERT()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "        loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "        loss_lm = (loss_lm.float()).mean()\n",
    "        loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
    "        loss = loss_lm + loss_clsf\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Predict mask tokens ans isNext\n",
    "    input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
    "    print(text)\n",
    "    print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
    "\n",
    "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "    logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "    print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
    "    print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "    logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "    print('isNext : ', True if isNext else False)\n",
    "    print('predict isNext : ',True if logits_clsf else False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello how are you i am romeo',\n",
       " 'hello romeo my name is juliet nice to meet you',\n",
       " 'nice meet you too how are you today',\n",
       " 'great my baseball team won the competition',\n",
       " 'oh congratulations juliet',\n",
       " 'thanks you romeo']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = (\n",
    "        'Hello, how are you? I am Romeo.\\n'\n",
    "        'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "        'Nice meet you too. How are you today?\\n'\n",
    "        'Great. My baseball team won the competition.\\n'\n",
    "        'Oh Congratulations, Juliet\\n'\n",
    "        'Thanks you Romeo'\n",
    "    )\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "\n",
    "tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a_index, tokens_b_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
